# -*- coding: utf-8 -*-
"""所有指標.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IuupF9GvOvMlW0m2Eq3U08EP-ZtQMotB
"""

!pip install pandas scikit-learn openpyxl numpy nltk
!pip install rouge-score
!pip install sentence-transformers
!pip install bert-score

import pandas as pd
import numpy as np
from sklearn.metrics import f1_score, recall_score
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
from sentence_transformers import SentenceTransformer
from bert_score import score as bert_score

# 讀取資料
df = pd.read_excel('/content/1030 turtleqa.xlsx')
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

model_cols = [col for col in df.columns if col not in ['Question', 'Truth Answer']]

# Semantic model, 建議中文用 'paraphrase-multilingual-MiniLM-L12-v2'
sbert_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

def simple_acc(truth, pred):
    return int(str(truth).strip() == str(pred).strip())

def bleu_score(truth, pred):
    smoothie = SmoothingFunction().method4
    truth_tokens = list(str(truth))
    pred_tokens = list(str(pred))
    return sentence_bleu([truth_tokens], pred_tokens, smoothing_function=smoothie)

def rouge_l_score(truth, pred):
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    return scorer.score(str(truth), str(pred))['rougeL'].fmeasure

def char_level_metrics(truth_list, pred_list):
    y_true_all, y_pred_all = [], []
    for t, p in zip(truth_list, pred_list):
        y_true = list(str(t).strip())
        y_pred = list(str(p).strip())
        all_tokens = set(y_true) | set(y_pred)
        y_true_bin = [1 if tk in y_true else 0 for tk in all_tokens]
        y_pred_bin = [1 if tk in y_pred else 0 for tk in all_tokens]
        y_true_all.append(y_true_bin)
        y_pred_all.append(y_pred_bin)
    f1_scores = [f1_score(y_true, y_pred, zero_division=0) for y_true, y_pred in zip(y_true_all, y_pred_all)]
    recall_scores = [recall_score(y_true, y_pred, zero_division=0) for y_true, y_pred in zip(y_true_all, y_pred_all)]
    return np.mean(f1_scores), np.mean(recall_scores)

def semantic_similarity(truth_list, pred_list):
    truth_embeds = sbert_model.encode(list(map(str, truth_list)), convert_to_tensor=True, show_progress_bar=False)
    pred_embeds = sbert_model.encode(list(map(str, pred_list)), convert_to_tensor=True, show_progress_bar=False)
    # 餘弦相似度，取每組平均
    sim_scores = (truth_embeds * pred_embeds).sum(axis=1) / (truth_embeds.norm(dim=1) * pred_embeds.norm(dim=1))
    return sim_scores.cpu().numpy().mean()

def bertscore_metric(truth_list, pred_list):
    # 用 BERTScore 對整組計算
    P, R, F1 = bert_score([str(p) for p in pred_list], [str(t) for t in truth_list], lang='zh', rescale_with_baseline=True)
    return F1.mean().item()

def compute_scores(truth_list, pred_list):
    exact_match_scores, bleu_scores, rouge_scores = [], [], []
    f1, recall = char_level_metrics(truth_list, pred_list)
    for t, p in zip(truth_list, pred_list):
        exact_match_scores.append(simple_acc(t, p))
        bleu_scores.append(bleu_score(t, p))
        rouge_scores.append(rouge_l_score(t, p))
    sem_sim = semantic_similarity(truth_list, pred_list)
    bert_score_val = bertscore_metric(truth_list, pred_list)
    return {
        "Accuracy": np.mean(exact_match_scores),
        "BLEU": np.mean(bleu_scores),
        "ROUGE-L": np.mean(rouge_scores),
        "F1": f1,
        "Recall": recall,
        "SemSim": sem_sim,
        "BERTScore": bert_score_val
    }

results = []
for model in model_cols:
    scores = compute_scores(df['Truth Answer'], df[model])
    scores['Model'] = model
    results.append(scores)

results_df = pd.DataFrame(results)
results_df = results_df[['Model', 'Accuracy', 'BLEU', 'ROUGE-L', 'F1', 'Recall', 'SemSim', 'BERTScore']]

results_df = results_df.round(4)  # 保留小數點後四位

print("模型分數比較：")
print(results_df.sort_values('ROUGE-L', ascending=False))

results_df.to_excel('model_scores.xlsx', index=False)